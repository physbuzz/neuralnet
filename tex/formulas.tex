%        File: formulas.tex
%     Created: Sun Nov 17 02:00 PM 2019 P
% Last Change: Sun Nov 17 02:00 PM 2019 P
%
\documentclass[letterpaper]{article}
\usepackage{amsmath}

\begin{document}
\section{Definitions}

All indexing is from 0. 
$L$ is the number of layers in the neural network, including the input and output layers (so the total number of hidden layers is $N-2$ and the total number of weight matrices is $N-1$). $n^{\ell}$ is the number of neurons in a given layer plus one. So, if there are 100 inputs, $n^0=101$. This extra ``neuron'' is always set to 1 to allow for biases to be handled succinctly. $\sigma$ is the activation function and $\tilde{L}$ is the loss function, and I put no assumptions on them.

\begin{align*}
    o_i^\ell&=\sigma(a_i^\ell) &&\text{(outputs)}\\
    a_i^\ell&=\sum_{i=0}^{n^\ell-1}o_j^{\ell-1}w_{ji}^{\ell-1} &&\text{(activation)}\\
    o_i^0&=\text{NN inputs}\\
    o_i^{L-1}&=\text{NN outputs}\\
    o_0^{\ell}&=1 && \text{(allows for biases)}\\
    w_{0i}^\ell&=\text{biases}\\
    w_{j0}^\ell&=\delta^{(\text{kronecker})}_{j0}\\
    \tilde{L}&=\tilde{L}(o_1^{L-1},\ldots,o_{n^{L-1}}^{L-1}) & & \text{(Loss)}\\
    \delta_i^\ell=-\frac{\partial \tilde{L}}{\partial a_i^\ell}
\end{align*}

\section{Forward Phase}

\begin{align*}
o_j^{\ell+1}&=\sigma\left(\sum_{i=0}^{n^\ell} o_i^{\ell} w_{ij}^\ell\right) & & (1\leq i < n^{L-1})\\
o_j^{0}&=I_j^d& & \text{(base case)}
\end{align*}
where $o^\ell_0=1$ always.

\section{Backwards Phase Derivations}

{\bf Theorem.} $$\frac{\partial \tilde L}{\partial  w_{ij}^\ell}=\delta_{j}^{\ell+1} o_i^\ell$$.

{\bf Proof.} First, consider the definition of $a_k^{\ell+1}=\sum_m o_m^\ell w_{mk}^\ell$. 
Then its partial derivative with respect to some $w_{ij}^\ell$ is trivial:

$$\frac{\partial a_k^{\ell+1}}{\partial w_{ij}^\ell}=o_i^\ell \delta_{jk}^{\text{(kronecker)}}$$

Now it's easy to compute:
\begin{align*}
    \frac{\partial \tilde L}{\partial  w_{ij}^\ell}&=\sum_k\frac{\partial \tilde L}{\partial a_k^{\ell+1}}\frac{\partial a_k^{\ell+1}}{\partial w_{ij}^\ell}\\
    &=\sum_k\delta_k^{\ell+1} o_i^\ell \delta_{jk}^{\text{(kronecker)}}\\
    &=\delta_j^{\ell+1} o_i^\ell \\
\end{align*}

{\bf Theorem. (Backpropagation Formula)} $$\delta_i^\ell=\sigma'(a_i^\ell)\sum_k w_{ik}^\ell \delta_k^{\ell+1}$$

{\bf Proof.} First note that, like for $\frac{\partial a_i^{\ell+1}}{\partial w_{ij}^\ell}$, we have:
$$\frac{\partial o_j^\ell}{\partial a_i^\ell}=\sigma'(a_i^\ell) \delta_{ij}^k$$
This implies a simple formula for $\frac{\partial a_k^{\ell+1}}{\partial a_i^\ell}$: 

\begin{align*}
    \frac{\partial a_k^{\ell+1}}{\partial a_i^\ell}&=\frac{\partial}{\partial a_i^\ell}\sum_j o_j^{\ell} w_{jk}^{\ell} \\
    &=sigma'(a_i^\ell) \delta_{ij}^k
\end{align*}<++>

$$$$
Then we can expand:

\begin{align*}
    \delta_i^\ell&=\frac{\partial \tilde L}{\partial  a_i^\ell}\\
    &=\sum_k\frac{\partial \tilde L}{\partial a_k^{\ell+1}}\frac{\partial a_k^{\ell+1}}{\partial a_{i}^\ell}\\
    &=\sum_k\delta_k^{\ell+1}\frac{\partial a_k^{\ell+1}}{\partial a_{i}^\ell}\\
    &=\sum_k\delta_k^{\ell+1} o_i^\ell \delta_{jk}^{\text{(kronecker)}}\\
    &=\delta_j^{\ell+1} o_i^\ell \\
\end{align*}




\section{Backwards Phase}

Base case:

\begin{align*}
    \delta_j^{L-1}&=o_j^{L-1}(1-o_j^{L-1})(o_j^{L-1}-D_j^d) & & (0\leq j < n^{L-1})\\
    \delta_j^\ell&=o_j^\ell(1-o_j^\ell)\sum_k w_{jk}^{\ell}\delta_k^{\ell+1} & & 0 \leq j < n^\ell
\end{align*}

Note that this is kind of funky for the term which would affect the biases, $\delta_0^\ell$. because 
$o_0^\ell=1$, $\delta_0^\ell=0$ always.

\section{Stepping}
$$\Delta w_{ij}^{\ell}=-\alpha \delta_j^{\ell+1} o_i^{\ell}$$

Note that $\Delta w_{i0}^\ell$ will always evaluate to zero. So this column is never changed.
But $\Delta_{0 j}^\ell$ can evaluate to nonzero values, so the biases are indeed updated.


\end{document}




